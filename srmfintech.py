# -*- coding: utf-8 -*-
"""SRMFinTech.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1442sqsbqbevy1FX9HaB7bFalHufPbhIq
"""

import pandas as pd

data=pd.read_csv("fintech3.csv")

data.shape

data.describe()

data.info()

data.head()

data.Gender.value_counts()

data.Demographic.value_counts()

data.Marital_status.value_counts()

data.Properties.value_counts()

data.Fin_Cat.value_counts()

data.Emp_status.value_counts()

def variables_types(data):
    cat_var = []
    non_cat_var = []
    dis_var = []
    con_var = []

    for column in data.columns:
        if data[column].dtype == "object":
            if data[column].nunique() < 10:
                cat_var.append(column)
            else:
                non_cat_var.append(column)
        elif data[column].dtype in ['int64', 'float64']:
            if data[column].nunique() < 10:
                dis_var.append(column)
            else:
                con_var.append(column)

    return cat_var, non_cat_var, dis_var, con_var

cat, non_cat, dis, con = variables_types(data)

print("Categorical variables:\n", cat)

print("\nNon-categorical variables:\n", non_cat)

print("\nDiscrete variables:\n", dis)

print("\nContinuous variables:\n", con)

data.describe(exclude='object').columns

data.describe(include='object').columns

data['Properties'].fillna(0, inplace=True)

data.head()

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
data['Gender']=data['Gender'].map({'Male':1,'Female':0})
data['Insurance']=data['Insurance'].map({'Yes':1,'No':0})
data['Demographic']=data['Demographic'].map({'Rural':0,'Suburban':1,'Urban':2})
data['Marital_status']=data['Marital_status'].map({'Married':1,'Single':0})
data['Properties']=data['Properties'].map({'Apartment':1,'Condo':2,'House':3})
data['Emp_status']=data['Emp_status'].map({'Self Employed':1,'Employee':2,'Entrepreneur':3})

data.head()

data['Properties'].fillna(0, inplace=True)

data.head()

data['Fin_Cat'].value_counts().plot(kind='barh');

data['Risk_tolerance'].value_counts().plot(kind='barh');

X = data.drop("Fin_Cat", axis=1)
y = data["Fin_Cat"]

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(X.columns)

import seaborn as sns
import matplotlib.pyplot as plt

sns.heatmap(data.corr(),annot=True,cmap='Blues')
plt.title('correlation of Finance Data')
plt.show()

from sklearn.linear_model import LogisticRegression
logreg_model = LogisticRegression()
logreg_model.fit(X_train_scaled, y_train)

from sklearn.metrics import accuracy_score
X_test_scaled = scaler.transform(X_test)
y_pred = logreg_model.predict(X_test_scaled)
accuracy = accuracy_score(y_test, y_pred)
print("Logistic Regression Accuracy:", accuracy)

from sklearn.neighbors import KNeighborsClassifier
knn_model = KNeighborsClassifier(n_neighbors=5)
knn_model.fit(X_train_scaled, y_train)

from sklearn.metrics import accuracy_score
X_test_scaled = scaler.transform(X_test)
y_pred = knn_model.predict(X_test_scaled)
accuracy_knn = accuracy_score(y_test, y_pred)
print("knn Accuracy:", accuracy_knn)

from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score
nb_classifier = GaussianNB()
nb_classifier.fit(X_train_scaled, y_train)
y_pred_nb = nb_classifier.predict(X_test_scaled)
accuracy_nb = accuracy_score(y_test, y_pred_nb)
print("Naive Bayes Accuracy:", accuracy_nb)

from sklearn.tree import DecisionTreeClassifier
dt_classifier = DecisionTreeClassifier(random_state=42)
dt_classifier.fit(X_train_scaled, y_train)

from sklearn.metrics import accuracy_score
y_pred_dt = dt_classifier.predict(X_test_scaled)
accuracy_dt = accuracy_score(y_test, y_pred_dt)
print("Decision Tree Accuracy:", accuracy_dt)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
rf_classifier = RandomForestClassifier(random_state=42)
rf_classifier.fit(X_train_scaled, y_train)
y_pred_rf = rf_classifier.predict(X_test_scaled)
accuracy_rf = accuracy_score(y_test, y_pred_rf)
print("Random Forest Accuracy:", accuracy_rf)

import matplotlib.pyplot as plt
from sklearn.tree import plot_tree

plt.figure(figsize=(12, 8))
plot_tree(rf_classifier.estimators_[0], feature_names=X.columns, filled=True, rounded=True)
plt.title("Random Forest Visualization")
plt.show()

import numpy as np

feature_importances = rf_classifier.feature_importances_

feature_names = X.columns
indices = np.argsort(feature_importances)[::-1]

plt.figure(figsize=(12, 8))
plt.bar(range(X.shape[1]), feature_importances[indices], align="center")
plt.xticks(range(X.shape[1]), feature_names[indices], rotation=45)
plt.title("Feature Importances - Random Forest")
plt.show()

data.tail()

input_data = (38,1,68000,14000,9500,3000,3,1,730,2,4,1,3,2)
input_np_data = np.asarray(input_data)
input_data_reshaped = input_np_data.reshape(1, -1)
std_data = scaler.transform(input_data_reshaped)
predicted_outcome = rf_classifier.predict(std_data)
print("Predicted Outcome:", predicted_outcome)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, BatchNormalization
from tensorflow.keras.layers import LeakyReLU


model = Sequential()
model.add(Dense(units=512, input_dim=X_train_scaled.shape[1]))
model.add(LeakyReLU(alpha=0.01))
model.add(BatchNormalization())
model.add(Dense(units=256))
model.add(LeakyReLU(alpha=0.01))
model.add(BatchNormalization())
model.add(Dense(units=128))
model.add(LeakyReLU(alpha=0.01))
model.add(Dense(units=64))
model.add(LeakyReLU(alpha=0.01))
model.add(Dense(units=1, activation='linear'))

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

model.fit(X_train_scaled, y_train, batch_size=40, epochs=25)

loss, accuracy = model.evaluate(X_test_scaled, y_test)
print("Test Loss:", loss)
print("Test Accuracy:", accuracy)

input_data = np.array([38,1,68000,14000,9500,3000,3,1,730,2,4,1,3,3])
input_data_scaled = scaler.transform(input_data.reshape(1, -1))
predicted_outcome_ann = model.predict(input_data_scaled)
predicted_outcome_ann_discrete = (predicted_outcome_ann > 0.5).astype(int)  # Apply thresholding
print("Predicted Outcome (ANN):", predicted_outcome_ann_discrete)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Dropout

X_train_reshaped = X_train_scaled.reshape(X_train_scaled.shape[0], 1, X_train_scaled.shape[1])
X_test_reshaped = X_test_scaled.reshape(X_test_scaled.shape[0], 1, X_test_scaled.shape[1])

model = Sequential()
model.add(LSTM(units=64, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=True))
model.add(Dropout(0.5))
model.add(LSTM(units=32))
model.add(Dropout(0.5))
model.add(Dense(units=1, activation='sigmoid'))

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

model.fit(X_train_reshaped, y_train, epochs=50, batch_size=32, validation_split=0.1, verbose=1)

loss, accuracy = model.evaluate(X_test_reshaped, y_test)
print("Test Loss:", loss)
print("Test Accuracy:", accuracy)

input_data = np.array([38,1,68000,14000,9500,3000,3,1,730,2,4,1,3,3]).reshape(1, 1, -1)
input_data_scaled = scaler.transform(input_data.reshape(1, -1))
predicted_outcome_rnn = model.predict(input_data_scaled.reshape(1, 1, -1))
print("Predicted Outcome (RNN):", predicted_outcome_rnn)

input_weights = model.layers[0].get_weights()[0]

feature_importances_rnn = np.sum(np.abs(input_weights), axis=1)

feature_names_rnn = X.columns

indices_rnn = np.argsort(feature_importances_rnn)[::-1]

plt.figure(figsize=(12, 8))
plt.bar(range(len(feature_importances_rnn)), feature_importances_rnn[indices_rnn], align="center")
plt.xticks(range(len(feature_importances_rnn)), feature_names_rnn[indices_rnn], rotation=45)
plt.title("Feature Importances - Deep Learning Model")
plt.show()

